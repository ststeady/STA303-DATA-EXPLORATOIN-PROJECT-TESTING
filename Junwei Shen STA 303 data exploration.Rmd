---
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
urlcolor: blue
header-includes:
- \usepackage{lastpage}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[CO, CE]{Junwei Shen, 1005244326}
- \fancyfoot[CO, CE]{\thepage \ of \pageref{LastPage}}
---

```{r setup, message = FALSE, echo=FALSE,warning=FALSE}

# Students: You probably shouldn't change any of the code in this chunk.

# These are the packages you will need for this activity
packages_needed <- c("tidyverse", "googledrive", "readxl", "janitor", 
                     "lubridate", "opendatatoronto", "ggthemes")

package.check <- lapply(
  packages_needed,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
    }
  }
)

# Credit: package.check based on a helpful post from Vikram Baliga https://vbaliga.github.io/verify-that-r-packages-are-installed-and-loaded/

# Load tidyverse
library(tidyverse)
library(readxl)
library(janitor)
library(opendatatoronto)
library(ggthemes)

# Set so that long lines in R will be wrapped:
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80), echo = FALSE)
```


```{r getdata, eval = FALSE, echo=FALSE}
# Students: You probably shouldn't change any of the code in this chunk BUT...

# This chunk loads the most recent data from Toronto City and the data from OpenToronto.

# You have to RUN this chunk by hand to update the data as 
#   eval is set to FALSE to limit unnecessary requsts on the site.

###################################################
# Step one: Get the COVID data from Toronto City. #
###################################################

googledrive::drive_deauth()

url1 <- "https://drive.google.com/file/d/11KF1DuN5tntugNc10ogQDzFnW05ruzLH/view"
googledrive::drive_download(url1, path="data/CityofToronto_COVID-19_Daily_Public_Reporting.xlsx", overwrite = TRUE)

url2 <- "https://drive.google.com/file/d/1jzH64LvFQ-UsDibXO0MOtvjbL2CvnV3N/view"
googledrive::drive_download(url2, path = "data/CityofToronto_COVID-19_NeighbourhoodData.xlsx", overwrite = TRUE)

# this removes the url object that we don't need anymore
rm(url1, url2)

#####################################################################
# Step two: Get the data neighbourhood data from Open Data Toronto. #
#####################################################################

nbhoods_shape_raw <- list_package_resources("neighbourhoods") %>% 
  get_resource()

saveRDS(nbhoods_shape_raw, "data/neighbourhood_shapefile.Rds")

nbhood_profile <- search_packages("Neighbourhood Profile") %>%
  list_package_resources() %>% 
  filter(name == "neighbourhood-profiles-2016-csv") %>% 
  get_resource()

saveRDS(nbhood_profile, "data/neighbourhood_profile.Rds")
```

```{r, echo=FALSE, message=FALSE}
setwd("C:/Users/User/Desktop/STA303 data exploration")
# Include this code only when the working directory shows a problem
```

```{r load_data, echo=FALSE}
######################################################
# Step three: Load the COVID data from Toronto City. #
######################################################

# Saving the name of the file as an object and then using the object name in the
# following code is a helpful practice. Why? If we change the name of the file 
# being used, we'll only have to change it in one place. This helps us avoid 
# 'human error'.

daily_data <- "data/CityofToronto_COVID-19_Daily_Public_Reporting.xlsx"

# Cases reported by date
reported_raw <- read_excel(daily_data, sheet = 5) %>% 
  clean_names()

# Cases by outbreak type
outbreak_raw <- read_excel(daily_data, sheet = 3) %>% 
  clean_names()

# When was this data updated?
date_daily <- read_excel(daily_data, sheet = 1) %>% 
  clean_names()

# By neighbourhood
neighbourood_data <- "data/CityofToronto_COVID-19_NeighbourhoodData.xlsx"

# Cases reported by date
nbhood_raw <- read_excel(neighbourood_data, sheet = 2) %>% 
  clean_names()

# Date the neighbourhood data was last updated
date_nbhood <- read_excel(neighbourood_data, sheet = 1) %>% 
  clean_names()

#don't need these anymore
rm(daily_data, neighbourood_data)

#############################################################
# Step four: Load the neighbourhood data from Toronto City. #
#############################################################

# Get neighbourhood profile data
nbhood_profile <- readRDS("data/neighbourhood_profile.Rds")

# Get shape data for mapping 
nbhoods_shape_raw <- readRDS("data/neighbourhood_shapefile.Rds") %>% 
  sf::st_as_sf() ## Makes sure shape info is in the most up to date format

```

Code last run `r Sys.Date()`.  
Daily:`r date_daily[1,1]`.   
Neighbourhood: `r date_nbhood[1,1]`. 

# Task 1: Daily cases
## Data wrangling

```{r cases_dw, echo=TRUE}
reported <- reported_raw %>% 
  mutate_if(is.numeric, replace_na, replace = 0)%>% # replacing all the NAs with 0
  mutate(reported_date = date(reported_date)) # represent date in Date format in R

# This data frame is not tidy so next we fix it.

reported <- reported %>%  
pivot_longer(-c(reported_date),
names_to = "status", values_to = "cases") 
# update the data frame to conclude all the status as a new column

reported$status <- recode_factor(reported$status, 
                                 "active" = "Active", 
                                 "recovered" = "Recovered", 
                                 "deceased"="Deceased")
reported<-reported %>% mutate(status = fct_relevel(status,"Active","Recovered","Deceased")) 
# relevel and renaming the "levels" in status

reported
```

\newpage
## Data visualization

```{r cases_vis,echo = TRUE}
reported %>%
ggplot(aes(x = reported_date,y = cases,fill = status))+
  geom_bar(stat = "identity")+
  scale_x_date(limits = c(date(("2020-01-01")), date(Sys.Date())),labels = scales::date_format("%d %b %y"))+
  scale_y_continuous(limits = c(0, 2000))+
  labs(title = "Cases reported by day in Toronto, Canada",
       subtitle = "Confirmed and probable cases",
       x = "Date",
       y = "Case count",
       caption = str_c("Created by: Junwei Shen for STA303/1002, U of T\nSource: Ontario Ministry of Health, Integrated Public Health Information System and CORES\n",date_daily[1,1]))+
  theme_minimal()+
  theme(legend.title = element_blank(), legend.position = c(.15, .8))+
  scale_fill_manual(values = c("#003F5C", "#86BCB6", "#B9CA5D"))
```

\newpage
# Task 2: Outbreak type
## Data wrangling


```{r outbreak_dw,echo = TRUE}
outbreak<-outbreak_raw %>% 
  mutate(episode_week = date(episode_week)) # represent date in Date format in R

total_table <- outbreak %>% 
  group_by(episode_week) %>% 
  summarize(total_cases = sum(cases), .groups = "drop")
outbreak <- left_join(outbreak,total_table) # create a new data frame to store the total cases, left join to update my outbreak data frame

# This data is tidy for creating the graph

outbreak$outbreak_or_sporadic <- recode_factor(outbreak$outbreak_or_sporadic, "OB Associated" = "Outbreak associated")
outbreak <- outbreak %>% 
  mutate(outbreak_or_sporadic = fct_relevel(outbreak_or_sporadic,"Sporadic","Outbreak associated"))
# relevel and renaming the "levels" in outbreak_or_sporadic

outbreak
```

\newpage
## Data visualization

```{r outbreak_vis, message = FALSE,echo = TRUE}
outbreak %>%
ggplot(aes(x = episode_week,y = cases,fill = outbreak_or_sporadic))+
  geom_bar(stat = "identity")+
  scale_x_date(limits = c(date(("2020-01-01")), date(Sys.Date()+7)),labels = scales::date_format("%d %b %y"))+
  scale_y_continuous(limits = c(0, max(outbreak$total_cases)))+
  labs(title = "Cases by outbreak type and week in Toronto, Canada",
       subtitle = "Confirmed and probable cases",
       x = "Date",
       y = "Case count",
       caption = str_c("Created by: Junwei Shen for STA303/1002,U of T\nSource: Ontario Ministry of Health, Integrated Public Health Information System and CORES\n",date_daily[1,1]))+
  theme_minimal()+
  theme(legend.title = element_blank(), legend.position = c(.15, .8))+
  scale_fill_manual(values = c("#86BCB6","#B9CA5D"))
```

\newpage
# Task 3: Neighbourhoods
## Data wrangling: part 1

```{r nbhood_dw_1,echo = TRUE}
checking_income <- nbhood_profile %>% 
  filter(grepl("Low income", Topic)) # checking how many rows are about low income
income <- checking_income %>% 
  filter(Characteristic == "  18 to 64 years (%)")
income <- income[2,c(6:146)] # choosing the id number 1143 column as it is a subgroup under the topic "LICO-AT", since the 1075 is misplaced
income <- as.data.frame(t(income))
income <- rownames_to_column(income) # turn this data frame tidy since the rows and columns are swapped

income <- income %>% 
  rename(neighbourhood_name = rowname,low_inc_percentage=V1)

income <- income %>% 
  mutate(low_inc_percentage = parse_number(low_inc_percentage)) # complete the data frame set up

glimpse(income)
```

## Data wrangling: part 2

```{r nbhood_dw_2,echo = TRUE}
nbhoods_all <- nbhoods_shape_raw %>% 
  mutate(neighbourhood_name = str_remove(AREA_NAME, "\\s\\(\\d+\\)$"))
nbhoods_all <- nbhoods_all %>% 
  mutate(neighbourhood_name = str_replace(neighbourhood_name,"St.James Town","St. James Town"))
nbhoods_all <- nbhoods_all %>% 
  mutate(neighbourhood_name = str_replace(neighbourhood_name,"Weston-Pellam Park","Weston-Pelham Park"))
# doing some cleaning on the names of the raw data

nbhoods_all <- nbhoods_all %>% 
  left_join(nbhood_raw, by = "neighbourhood_name") # left join matching by the neighbourhood_name
income <- income %>% filter(neighbourhood_name != "City of Toronto") # we don't want City of Toronto in this case as one of the regions
nbhoods_all <- nbhoods_all %>% 
  left_join(income,by = "neighbourhood_name")
nbhoods_all <- nbhoods_all %>% 
  rename(rate_per_100000 = rate_per_100_000_people)

glimpse(nbhoods_all)
```

## Data wrangling: part 3

```{r nbhood_dw_3,echo = TRUE}
med_rate = median(nbhoods_all$rate_per_100000) # calculating the median of cases rate per 100,000 people
med_inc = median(nbhoods_all$low_inc_percentage) # calculating the median of low income percentage 

nbhoods_final <- nbhoods_all %>%
  mutate(nbhood_type = case_when(
  low_inc_percentage >= med_inc & rate_per_100000 >= med_rate ~ "Higher low income rate, higher case rate",
  low_inc_percentage >= med_inc & rate_per_100000 < med_rate ~ "Higher low income rate, lower case rate",
  low_inc_percentage < med_inc & rate_per_100000 >= med_rate ~ "Lower low income rate, higher case rate",
  low_inc_percentage < med_inc & rate_per_100000 < med_rate ~ "Lower low income rate, lower case rate"))

glimpse(nbhoods_final)  
```

\newpage
## Data visualization

```{r neighbourhood_graphs_1, fig.height=4, echo = TRUE}
ggplot(data = nbhoods_final)+
  geom_sf(aes(fill = low_inc_percentage))+
  theme_map()+
  scale_fill_gradient(name = "% low income", low = "darkgreen", high = "lightgrey")+
  theme(legend.position = c(0.9,0.15))+
  labs(title = "Percentage of 18 to 64 year olds living in a low income family (2015)",
       subtitle = "Neighbourhoods of Toronto, Canada",
       caption = str_c("Created by: Junwei Shen for STA303/1002,U of T\nSource: Census Profile 98−316−X2016001 via Open\n",date_daily[1,1]))
```

\newpage

```{r neighbourhood_graphs_2, fig.height=4, echo = TRUE}
ggplot(data = nbhoods_final)+
  geom_sf(aes(fill = rate_per_100000))+
  theme_map()+
  scale_fill_gradient(name = "Cases per 100,000 people", low = "white", high = "darkorange")+
  theme(legend.position = c(0.8,0.1))+
  labs(title = "COVID−19 cases per 100,000, by neighbourhood in Toronto, Canada",
       caption = str_c("Created by: Junwei Shen for STA303/1002,U of T\nSource: Ontario Ministry of Health, Integrated Public Health Information System and CORES\n",date_daily[1,1]))
```

\newpage

```{r neighbourhood_graphs_3, fig.height=4, echo = TRUE}
ggplot(data = nbhoods_final)+
  geom_sf(aes(fill = nbhood_type))+
  theme_map()+
  scale_fill_brewer(name = "% of 18 to 64 year−olds in low income\nfamilies and COVID−19 case rates",palette = "Set1")+
  theme(legend.position = c(0.76,0),legend.title = element_text(size=8),legend.text = element_text(size=7.1))+
  labs(title = "COVID−19 cases per 100,000, by neighbourhood in Toronto, Canada",
       caption = str_c("Created by: Junwei Shen for STA303/1002,U of T\nIncome data source: Census Profile 98−316−X2016001 via OpenData Toronto\nCOVID data source: Ontario Ministry of Health, Integrated Public\nHealth Information System and CORES\n",date_daily[1,1]))
```




```{r, eval = FALSE}
# This chunk of code helps you prepare your assessment for submission on Crowdmark
# This is optional. If it isn't working, you can do it manually/take another approach.

# Run this chunk by hand after knitting your final version of your pdf for submission.
# A new file called 'to_submit' will appear in your working directory with each page of your assignment as a separate pdf.

# Install the required packages
if(!match("staplr", installed.packages()[,1], nomatch = FALSE))
  {install.packages("staplr")}

# Don't edit anything in this function
prep_for_crowdmark <- function(pdf=NULL){
  # Get the name of the file you're currently in. 
  this_file <- rstudioapi::getSourceEditorContext()$path
  pdf_name <- sub(".Rmd", ".pdf", sub('.*/', '', this_file))
  
  # Create a file called to_submit to put the individual files in
  # This will be in the same folder as this file is saved
  if(!match("to_submit", list.files(), nomatch = FALSE))
    {dir.create("to_submit")}
 
  # Split the files
  if(is.null(pdf)){
  staplr::split_pdf(pdf_name, output_directory = "to_submit", prefix = "page_")} else {
    staplr::split_pdf(pdf, output_directory = "to_submit", prefix = "page_") 
  }
}

prep_for_crowdmark()
```